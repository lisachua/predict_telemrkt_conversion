{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Final-Project-Check-in\" data-toc-modified-id=\"Final-Project-Check-in-1\">Final Project</a></span></li><li><span><a href=\"#Group-Name\" data-toc-modified-id=\"Group-Name-2\">Group Name</a></span></li><li><span><a href=\"#Student-Names\" data-toc-modified-id=\"Student-Names-3\">Student Names</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4\">Load Data</a></span></li><li><span><a href=\"#Fit-scikit-learn-model\" data-toc-modified-id=\"Fit-scikit-learn-model-5\">Fit scikit-learn model</a></span></li><li><span><a href=\"#Evaluation-Metric\" data-toc-modified-id=\"Evaluation-Metric-6\">Evaluation Metric</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Name: SAL\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to GitHub Repository: https://github.com/swbuchanan13/msds699_sal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Names\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lisa Chua\n",
    "2. Alaa A Latif\n",
    "3. Shane Buchanan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rfpimp import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import compose\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn import impute\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Pandas DataFrame\n",
    "data_url = \"https://raw.githubusercontent.com/swbuchanan13/msds699_sal/master/bank-additional-full.csv\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv(data_url, error_bad_lines=False, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  duration  campaign  pdays  previous     poutcome  \\\n",
       "0   may         mon       261         1    999         0  nonexistent   \n",
       "1   may         mon       149         1    999         0  nonexistent   \n",
       "2   may         mon       226         1    999         0  nonexistent   \n",
       "3   may         mon       151         1    999         0  nonexistent   \n",
       "4   may         mon       307         1    999         0  nonexistent   \n",
       "\n",
       "   emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "1           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "2           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "3           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "4           1.1          93.994          -36.4      4.857       5191.0  no  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Sample of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 int64\n",
       "job                object\n",
       "marital            object\n",
       "education          object\n",
       "default            object\n",
       "housing            object\n",
       "loan               object\n",
       "contact            object\n",
       "month              object\n",
       "day_of_week        object\n",
       "duration            int64\n",
       "campaign            int64\n",
       "pdays               int64\n",
       "previous            int64\n",
       "poutcome           object\n",
       "emp.var.rate      float64\n",
       "cons.price.idx    float64\n",
       "cons.conf.idx     float64\n",
       "euribor3m         float64\n",
       "nr.employed       float64\n",
       "y                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Data Type for each Variable\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: job\n",
      "2: marital\n",
      "3: education\n",
      "4: default\n",
      "5: housing\n",
      "6: loan\n",
      "7: contact\n",
      "8: month\n",
      "9: day_of_week\n",
      "14: poutcome\n"
     ]
    }
   ],
   "source": [
    "# Display index and name of each categorical feature\n",
    "t = (df.dtypes == 'object')\n",
    "trues = [i for i, x in enumerate(t) if x][:-1]\n",
    "for i in trues:\n",
    "    print(f'{i}: {df.columns[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = df.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Train-Test Split\n",
    "\n",
    "We perform a customized train-test split because the dataset is highly imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positively labeled samples: 4640\n",
      "Negatively labeled samples: 36548\n"
     ]
    }
   ],
   "source": [
    "pos_samples = df[df['y']=='yes'].shape[0]\n",
    "neg_samples = df[df['y']=='no'].shape[0]\n",
    "print(f\"Positively labeled samples: {pos_samples}\")\n",
    "print(f\"Negatively labeled samples: {neg_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate positive and negative samples\n",
    "df_yes = df[df['y']=='yes']\n",
    "df_no = df[df['y']=='no']\n",
    "# perform train-test split on each\n",
    "df_train_yes, df_test_yes = train_test_split(df_yes, test_size=0.2)\n",
    "df_train_no, df_test_no = train_test_split(df_no, test_size=0.2)\n",
    "# concatenate back to create a training and test set\n",
    "df_train = pd.concat([df_train_yes, df_train_no], axis=0)\n",
    "df_test = pd.concat([df_test_yes, df_test_no], axis=0)\n",
    "# separate features from \n",
    "X_train, y_train = df_train.drop('y', axis=1), df_train['y']\n",
    "X_test, y_test = df_test.drop('y', axis=1), df_test['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_sample(X, y):\n",
    "    rows_idxs_no = y == 'no'\n",
    "    rows_idxs_yes = y == 'yes'\n",
    "    df_yes = pd.concat((X.loc[rows_idxs_yes], y.loc[rows_idxs_yes]), axis = 1)\n",
    "    df_no = pd.concat((X.loc[rows_idxs_no], y.loc[rows_idxs_no]), axis = 1)\n",
    "    df_yes_upsampled = df_yes.sample(int(len(df_yes)*5), replace=True)\n",
    "    df_upsampled = pd.concat([df_no, df_yes_upsampled], axis=0)\n",
    "    return df_upsampled.drop('y', axis=1), df_upsampled['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTENC(trues)\n",
    "X_train_up, y_train_up = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_up = pd.DataFrame(X_train_up, columns=X.columns)\n",
    "y_train_up = pd.Series(y_train_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(classifier=None):\n",
    "    \"Create a single pipeline that processes the data and then fits the classifier.\" \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    numeric_features     = ['age', 'campaign', 'cons.price.idx', 'pdays', 'previous', 'emp.var.rate', 'cons.conf.idx',\n",
    "                            'euribor3m', 'nr.employed']\n",
    "    categorical_features = ['job', 'education', 'default', 'housing',\n",
    "                            'loan', 'campaign', 'poutcome', 'marital', 'contact', 'month', 'day_of_week']\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('impute', impute.SimpleImputer(strategy='most_frequent', add_indicator=True)),\n",
    "    ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', impute.SimpleImputer(strategy='median')),\n",
    "    ('scaler', preprocessing.StandardScaler())])\n",
    "    \n",
    "    \n",
    "    \n",
    "    preprocessor = compose.ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', classifier)])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dtree and fit.\n",
    "base_clf = DecisionTreeClassifier()\n",
    "base_pipeline = make_pipeline(base_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Estimator\n",
    "base_pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Test set\n",
    "y_preds = base_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833818888079631"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate \n",
    "# Use accuracy score as a basic evaluation metric\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8383453110419161"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate \n",
    "# Use F1 score as metric since we have an unbalanced data set.\n",
    "f1_score(y_test, y_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_model_selection():\n",
    "    pipelines = {}\n",
    "    models = {}\n",
    "    models['logistic'] = LogisticRegression(solver='sag')\n",
    "    models['random forest'] = RandomForestClassifier()\n",
    "    models['GBM'] = GradientBoostingClassifier()\n",
    "    models['SVM'] = SVC()\n",
    "    print('Model: \\t  \\t Accuracy \\t F1 Score')\n",
    "    for name in models:\n",
    "        clf = models[name]\n",
    "        clf_pipeline = make_pipeline(clf)\n",
    "        clf_pipeline.fit(X_train, y_train);\n",
    "        y_preds = clf_pipeline.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_preds)\n",
    "        f1 = f1_score(y_test, y_preds, average='weighted')\n",
    "        print(f'{name}: \\t  \\t {acc:.2f} \\t {f1:.2f}')\n",
    "        pipelines[name] = clf_pipeline\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t  \t Accuracy \t F1 Score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaa/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic: \t  \t 0.90 \t 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaa/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest: \t  \t 0.89 \t 0.87\n",
      "GBM: \t  \t 0.90 \t 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alaa/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipelines = do_model_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 8]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scoring function to pass to the Randomized Search\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 5, verbose=2, \n",
    "                               scoring=f1_scorer, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = make_pipeline(rf_random)\n",
    "rf_pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_clf = rf_pipeline['classifier']\n",
    "tuned_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = RandomForestClassifier(n_estimators = 500, min_samples_split = 2, \n",
    "                        min_samples_leaf = 2, max_features = 'sqrt',\n",
    "                        bootstrap = True)\n",
    "best_pipeline = make_pipeline(best_clf)\n",
    "best_pipeline.fit(X_train, y_train);\n",
    "y_preds = best_pipeline.predict(X_test)\n",
    "print(f\"F1 Test Score: {f1_score(y_test, y_preds, average='weighted'):.2f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_preds):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "### KEY TAKEAWAYS\n",
    "\n",
    "1. <b>Intro:</b> Since our domain is business intelligence for improving tele-marketing strategy, we will focus heavily on feature importance to understand which factors play a bigger role in making the sale. \n",
    "\n",
    "\n",
    "2. <b>Overview of methods:</b> We tried three different feature importance methods: drop column, permutation and RFPimp's importance algorithm (which basically implements permutation?). We can see from the heat map that alot of the features are correlated/codependent. Therefore we decided to use the RFPimp implementation as it allows us to consider multiple features together. It avoids showing low importance for codependent features as with drop column, or having codepedent features share importance scores as with permutation.\n",
    "\n",
    "\n",
    "3. <b>Interpret results:</b> The RFPimp results show <b>nr.employed, cons.conf.indx, euribor3m, month_oct</b> as the most important features. We notice that the top three features are all economic indicators. This finding may have business implications, for example:\n",
    "\n",
    "    - Telemarketing might just not be effective in bad economic conditions and the bank should try different marketing techniques.\n",
    "    - The bank should collect more economic indicators as they seem to be important factors in whether a sale is made.\n",
    "\n",
    "\n",
    "4. <b>Next steps:</b> Our pipelines current model using all features, our next steps are to do more feature selection based on the results of these feature importance algorithms and to rerun our models to see if we can improve f1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features data, drop y and Duration.\n",
    "X = df.drop(['duration','y'], axis=1)\n",
    "# One-hot encode categorical features.\n",
    "X_dumm = pd.get_dummies(X)\n",
    "# Labels data.\n",
    "y = df['y']\n",
    "y = y.map({'no': 0, 'yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test-split on data.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dumm, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFPIMP\n",
    "rf = RandomForestRegressor(n_estimators = 100, n_jobs=-1, oob_score=True)\n",
    "rf.fit(X_train, y_train)\n",
    "imp_rfpimp = importances(rf, X_val, y_val)\n",
    "viz = plot_importances(imp_rfpimp)\n",
    "viz.save('file.svg')\n",
    "viz.view() # view viz in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFPIMP heatmap\n",
    "df_train = feature_corr_matrix(X)\n",
    "heatmap = plot_corr_heatmap(df, figsize=(8,8))\n",
    "heatmap.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the class determination point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_y = best_pipeline.predict_proba(X_test)[:,1]\n",
    "y_pred_scale = np.where((probs_y > .2), 'yes', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Citation:\n",
    "    Function from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_scale, normalize=True,\n",
    "                      title='Business adjusted Normalized confusion matrix')\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True,\n",
    "                      title='Normalized confusion matrix for Generic RF')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = pd.Series(y_test).replace({'no': 0, 'yes': 1})\n",
    "y_pred_binary = pd.Series(y_pred).replace({'no': 0, 'yes': 1})\n",
    "lr_probs = best_pipeline.predict_proba(X_test)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test_binary, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test_binary, y_pred_binary, average='weighted'), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Random Forest: f1=%.2f auc=%.2f' % (round(lr_f1, 2), lr_auc))\n",
    "# plot the precision-recall curves\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Random Forest')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways:\n",
    "Feature importance found that most of our variables are not predictive of conversion. When we compared our best f1 score to benchmarks and high scores on Kaggle, we hit a ceiling of 0.88, which may further indicate that there are missing features. Another limitation is that naïve and SMOTE upsampling to address our imbalanced data problem did not improve our model metrics. Some potential next steps would be to rerun models after feature selection and collect more economic features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
